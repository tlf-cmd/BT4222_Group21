{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludsil/miniconda3/envs/prosam/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/esg_reports/ABT_Abbott/ABT_2021.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = {}\n",
    "\n",
    "for folder in os.listdir('./data/esg_reports'):\n",
    "    for file in os.listdir(f'./data/esg_reports/{folder}'):\n",
    "        if file.endswith('2021.txt'):\n",
    "            df = pd.read_csv(f'./data/esg_reports/{folder}/{file}')\n",
    "            # pick out ticker\n",
    "            ticker = folder.split('_')[0]\n",
    "            reports[ticker] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_abt = reports['ABT']\n",
    "# df_abt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BSX', 'BDX', 'EW', 'BAX', 'ISRG', 'MDT', 'DXCM', 'PODD', 'SYK', 'HOLX', 'RMD', 'ABT', 'COO', 'IDXX', 'TFX', 'XRAY', 'ZBH'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topics as keys, and a list of all sentences classified as that topic as values\n",
    "\n",
    "topics_by_company = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "for company in reports:\n",
    "\n",
    "    sentences_by_topic = defaultdict(lambda: [])\n",
    "\n",
    "    for index, row in reports[company].iterrows():\n",
    "        if row['ESG BERT Topic 1'][0] != 'NON-ESG':\n",
    "            input_string = row['ESG BERT Topic 1']\n",
    "            match = re.match(r\"\\('(.+)',\\s*(\\d+\\.\\d+)\\)\", input_string)\n",
    "            topic = match.group(1)\n",
    "            number = float(match.group(2))\n",
    "            sentences_by_topic[topic].append((row['Sentence'], number))\n",
    "    topics_by_company[company] = sentences_by_topic\n",
    "\n",
    "# Funkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort sentences by topic by the number\n",
    "for company in topics_by_company:\n",
    "    for topic in topics_by_company[company]:\n",
    "        sentences_by_topic = topics_by_company[company]\n",
    "        sentences_by_topic[topic] = sorted(sentences_by_topic[topic], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Funkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_list = []\n",
    "# # Add the 10 first sentences of 'Access_And_Affordability' to the document\n",
    "# for sentence, number in sentences_by_topic['Access_And_Affordability'][:10]:\n",
    "#     sentence_list.append(sentence)\n",
    "\n",
    "# document = \" \".join(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = transformers.pipeline('summarization')\n",
    "# summary = summarizer(document, max_length=100, min_length=10, do_sample=False)\n",
    "# print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business_Model_Resilience :   2021 Performance Report: Transforming care Investing in   our people Accelerating   possibilities Protecting the  environment Creating value  responsibly . Good governance  reflects our values  Compliance, ethics  and integrity Protecting human rights  across our company  and supply chain Keeping our supply chain  resilient and reliable Risk management  and global security Keeping our products   and patient data secure .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product_Design_And_Lifecycle_Management :   We have made measurable progress with  initiatives focused on packaging, labeling and device recycling . We are focused on further reducing environmental  impacts of our technologies throughout the entire product life cycle .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee_Health_And_Safety :   For 2021, Boston Scientific expanded the relevant headcount in scope for TRIR, TRIFR, LTIFR  and OLTR reporting to include 98% of our population . 2021 Performance Report: Transforming care Investing in our people Accelerating   possibilities Protecting the  environment Creating value  responsibly .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee_Engagement_Inclusion_And_Diversity :   Boston Scientific increased representation of women in supervisor and manager roles globally to 41.1% and multicultural employees in the U.S. and Puerto Rico to 21.6% In 2021, we made progress toward our goals by taking the following actions .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2277 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m document \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(sentence_list)\n\u001b[1;32m     14\u001b[0m summarizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mpipeline(\u001b[39m'\u001b[39m\u001b[39msummarization\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m summary \u001b[39m=\u001b[39m summarizer(document, max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, min_length\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(topic, \u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m , summary[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msummary_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:265\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    242\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[1;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[1;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[1;32m    170\u001b[0m     ):\n\u001b[1;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/pipelines/base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1102\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1103\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         )\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/pipelines/base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1115\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1116\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1117\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1014\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1015\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1016\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/generation/utils.py:1268\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1261\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1263\u001b[0m         )\n\u001b[1;32m   1265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1266\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1268\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1269\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1270\u001b[0m     )\n\u001b[1;32m   1272\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/generation/utils.py:634\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    632\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    633\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 634\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    636\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:810\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale\n\u001b[0;32m--> 810\u001b[0m embed_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_positions(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    811\u001b[0m embed_pos \u001b[39m=\u001b[39m embed_pos\u001b[39m.\u001b[39mto(inputs_embeds\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    813\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m embed_pos\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:138\u001b[0m, in \u001b[0;36mBartLearnedPositionalEmbedding.forward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    133\u001b[0m bsz, seq_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    134\u001b[0m positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\n\u001b[1;32m    135\u001b[0m     past_key_values_length, past_key_values_length \u001b[39m+\u001b[39m seq_len, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    136\u001b[0m )\u001b[39m.\u001b[39mexpand(bsz, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(positions \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moffset)\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/prosam/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# For some reason still uses abbott first?\n",
    "\n",
    "for company in topics_by_company:\n",
    "    print(company)\n",
    "    for topic in topics_by_company[company]:\n",
    "        sentence_list = []\n",
    "        sentences_by_topic = topics_by_company[company]\n",
    "        for sentence, number in sentences_by_topic[topic][:5]:\n",
    "            # print(topic, sentence, number)\n",
    "            sentence_list.append(sentence)\n",
    "\n",
    "        document = \" \".join(sentence_list)\n",
    "\n",
    "        summarizer = transformers.pipeline('summarization')\n",
    "        summary = summarizer(document, max_length=100, min_length=10, do_sample=False)\n",
    "        print(topic, \": \" , summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f00d82125af2013b743adbe5664b563fb3fff0416a803f3f6c5f16aed73ac2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
